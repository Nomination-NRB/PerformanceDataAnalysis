{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyTestTPath='data/key_test_t.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divideFile(output_dir = 'data/small_files'):\n",
    "    # 判断输出目录下是否存在文件，如果存在则删除\n",
    "    if os.path.exists(output_dir):\n",
    "        for file in os.listdir(output_dir):\n",
    "            os.remove(os.path.join(output_dir, file))\n",
    "    else:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    num_files = 100\n",
    "    # 列名或列索引列表，只保留需要的列\n",
    "    keep_columns = ['results', 'dimension']\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv(keyTestTPath)\n",
    "\n",
    "    chunk_size = len(df) // num_files\n",
    "\n",
    "    for i in range(num_files):\n",
    "        # 计算当前小文件的起始行和结束行\n",
    "        start = i * chunk_size\n",
    "        end = (i + 1) * chunk_size if i < num_files - 1 else len(df)\n",
    "        \n",
    "        # 提取当前小文件的数据\n",
    "        small_df = df.iloc[start:end, :][keep_columns]\n",
    "        \n",
    "        # 将当前小文件保存为CSV文件\n",
    "        file_name = f'small_file_{i + 1}.csv'\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "        small_df.to_csv(file_path, index=False)\n",
    "        if i % 10 == 0:\n",
    "            print(f'Small file {file_name} has been saved.')\n",
    "\n",
    "    print('All files have been saved.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divideFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def judgeJson(keyTestT):\n",
    "    for i in range(len(keyTestT)):\n",
    "        try:\n",
    "            json.loads(keyTestT.loc[i, 'dimension'])\n",
    "            json.loads(keyTestT.loc[i, 'results'])\n",
    "        except:\n",
    "            keyTestT.drop(i, inplace=True)\n",
    "    keyTestT = keyTestT.reset_index(drop=True)\n",
    "    return keyTestT\n",
    "\n",
    "\n",
    "def filterDataByKey(keyTestT, keyName):\n",
    "    correctNum=0\n",
    "    for i in range(len(keyTestT)):\n",
    "        results = json.loads(keyTestT.loc[i, 'results'])\n",
    "        # 将resuls转为字符串，使用正则表达式判断是否包含keyName\n",
    "        resultsStr = str(results)\n",
    "        if re.search(keyName, resultsStr):\n",
    "            correctNum+=1\n",
    "    return correctNum\n",
    "\n",
    "\n",
    "def getResultArgList(keyTestT):\n",
    "    resultArgList = []\n",
    "    for i in range(len(keyTestT)):\n",
    "        results = json.loads(keyTestT.loc[i, 'results'])\n",
    "        # 将resuls转为字符串，使用正则表达式判断是否包含#字符，若包含则将#字符串提取出来\n",
    "        resultsStr = str(results)\n",
    "        resultArg = re.findall(r'#\\w+', resultsStr)\n",
    "        resultArgList.extend(resultArg)\n",
    "    # 去重\n",
    "    resultArgList = list(set(resultArgList))\n",
    "    print('arg num:', len(resultArgList))\n",
    "    # print('five for example:', resultArgList[:5])\n",
    "    return keyTestT,resultArgList\n",
    "\n",
    "\n",
    "def getNumOfKey(keyTestT, resultArgList):\n",
    "    keyNumList = []\n",
    "    # 调用filterDataByKey函数，统计每个key的数量\n",
    "    for key in resultArgList:\n",
    "        keyNumList.append(filterDataByKey(keyTestT, key))\n",
    "    return keyNumList\n",
    "\n",
    "\n",
    "# 文件夹路径\n",
    "smallFilePath = 'data/small_files/'\n",
    "# 文件名的通配符\n",
    "file_pattern = 'small_file_*.csv'\n",
    "# 获取文件夹下所有符合通配符的文件路径\n",
    "file_paths = glob.glob(smallFilePath + file_pattern)\n",
    "\n",
    "# 定义处理单个文件的函数\n",
    "def process_csv_file(file_path):\n",
    "    # 读取当前文件的数据\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = judgeJson(df)\n",
    "    df, resultArgList = getResultArgList(df)\n",
    "    keyNumList = getNumOfKey(df, resultArgList)\n",
    "    keyNumDict = dict(zip(resultArgList, keyNumList))\n",
    "    return df, keyNumDict\n",
    "\n",
    "# 使用ThreadPoolExecutor并行处理多个文件\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=15) as executor:\n",
    "    futures = {executor.submit(process_csv_file, file_path): file_path for file_path in file_paths}\n",
    "    combined_keyNumDict = {}\n",
    "    dfs = []\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            df, keyNumDict = future.result()\n",
    "            combined_keyNumDict.update(keyNumDict)\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f'Error processing file: {futures[future]}, {e}')\n",
    "\n",
    "\n",
    "df_combined = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df_combined.to_csv('data/combined_small_files.csv', index=False)\n",
    "\n",
    "print(combined_keyNumDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "small_file_path = 'data/small_files/'\n",
    "\n",
    "# 定义处理单个文件的函数\n",
    "def process_csv_file(file_path):\n",
    "    # 读取当前文件的数据\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = judgeJson(df)\n",
    "    df, resultArgList = getResultArgList(df)\n",
    "    keyNumList = getNumOfKey(df, resultArgList)\n",
    "    keyNumDict = dict(zip(resultArgList, keyNumList))\n",
    "    return df, keyNumDict\n",
    "\n",
    "# 获取文件夹下所有文件的路径\n",
    "file_paths = [os.path.join(small_file_path, file) for file in os.listdir(small_file_path) if file.endswith('.csv')]\n",
    "\n",
    "# 顺序执行文件，并在处理完每个文件后合并 keyNumDict\n",
    "combined_keyNumDict = {}\n",
    "dfs = []\n",
    "for file_path in file_paths:\n",
    "    df, keyNumDict = process_csv_file(file_path)\n",
    "    combined_keyNumDict.update(keyNumDict)\n",
    "    dfs.append(df)\n",
    "\n",
    "# 将所有小文件的结果合并为一个 DataFrame\n",
    "df_combined = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# 将合并后的结果保存为一个 CSV 文件\n",
    "df_combined.to_csv('data/combined_small_files.csv', index=False)\n",
    "\n",
    "# 打印合并后的总字典\n",
    "print(combined_keyNumDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyTestT=pd.read_csv(keyTestTPath,usecols=['results','dimension','results_key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judgeJson(keyTestT):\n",
    "    for i in tqdm.tqdm(range(len(keyTestT))):\n",
    "        try:\n",
    "            json.loads(keyTestT.loc[i, 'dimension'])\n",
    "            json.loads(keyTestT.loc[i, 'results'])\n",
    "        except:\n",
    "            keyTestT.drop(i, inplace=True)\n",
    "    keyTestT = keyTestT.reset_index(drop=True)\n",
    "    return keyTestT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyTestT=judgeJson(keyTestT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyTestT.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filterDataByKey(keyTestT, keyName):\n",
    "#     correctNum=0\n",
    "#     for i in range(len(keyTestT)):\n",
    "#         results = json.loads(keyTestT.loc[i, 'results'])\n",
    "#         for result in results:\n",
    "#             if keyName in result:\n",
    "#                 correctNum+=1\n",
    "#                 break\n",
    "#     return correctNum\n",
    "\n",
    "\n",
    "def filterDataByKey(keyTestT, keyName):\n",
    "    correctNum=0\n",
    "    for i in range(len(keyTestT)):\n",
    "        results = json.loads(keyTestT.loc[i, 'results'])\n",
    "        # 将resuls转为字符串，使用正则表达式判断是否包含keyName\n",
    "        resultsStr = str(results)\n",
    "        if re.search(keyName, resultsStr):\n",
    "            correctNum+=1\n",
    "    return correctNum\n",
    "\n",
    "\n",
    "def getResultArgList(keyTestT,resultArgList):\n",
    "    for index, row in tqdm.tqdm(keyTestT.iterrows(), total=keyTestT.shape[0]):\n",
    "        result = row['results']\n",
    "        resultJson = json.loads(result)\n",
    "        for key in resultJson:\n",
    "            if key.find('#') != -1:\n",
    "                resultArgList.append(key)\n",
    "\n",
    "    resultArgList = list(set(resultArgList))\n",
    "    print('arg num:', len(resultArgList))\n",
    "    print('five for example:', resultArgList[:5])\n",
    "    return keyTestT,resultArgList\n",
    "\n",
    "\n",
    "def getNumOfKey(keyTestT, resultArgList):\n",
    "    keyNumList = []\n",
    "    # 调用filterDataByKey函数，统计每个key的数量\n",
    "    for key in resultArgList:\n",
    "        keyNumList.append(filterDataByKey(keyTestT, key))\n",
    "    return keyNumList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultArgList=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyTestT, resultArgList = getResultArgList(keyTestT,resultArgList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyNumList = getNumOfKey(keyTestT, resultArgList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showInfoList(yourlist):\n",
    "    # 展示列表的信息\n",
    "    print('max:', max(yourlist))\n",
    "    print('min:', min(yourlist))\n",
    "    print('median:', np.median(yourlist))\n",
    "    print('len:', len(yourlist))\n",
    "    return max(yourlist), min(yourlist), np.median(yourlist)\n",
    "\n",
    "def getTreshold(yourlist):\n",
    "    # 获取阈值\n",
    "    yourMax, yourMin, youMdian = showInfoList(yourlist)\n",
    "    diff = yourMax - youMdian\n",
    "    thresholds = [youMdian + 0.1 * diff, youMdian + 0.2 * diff, youMdian + 0.3 * diff]\n",
    "    print(\"\\nPossible thresholds:\")\n",
    "    for threshold in thresholds:\n",
    "        print(threshold)\n",
    "    return thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treshold=getTreshold(keyNumList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画图\n",
    "plt.figure(figsize=(90, 20))\n",
    "plt.bar(range(len(keyNumList)), keyNumList)\n",
    "plt.xticks(range(len(keyNumList)), resultArgList, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMostImportantKey(resultArgList, keyNumList, threshold):\n",
    "    # 获取重要的key\n",
    "    importantKeyList = []\n",
    "    for i in range(len(keyNumList)):\n",
    "        if keyNumList[i] > threshold:\n",
    "            importantKeyList.append(resultArgList[i])\n",
    "    return importantKeyList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importantKeyList = getMostImportantKey(resultArgList, keyNumList, max(treshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImportantKeyDict(resultArgList,keyNumList,importantKeyList):\n",
    "    # 获取重要key的字典\n",
    "    importantKeyDict = {}\n",
    "    for i in range(len(importantKeyList)):\n",
    "        importantKeyDict[importantKeyList[i]] = keyNumList[resultArgList.index(importantKeyList[i])]\n",
    "    return importantKeyDict\n",
    "\n",
    "def showDict(yourDict):\n",
    "    # 展示字典\n",
    "    for key in yourDict:\n",
    "        print(key, ':', yourDict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importantKeyDict=getImportantKeyDict(resultArgList,keyNumList,importantKeyList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showDict(importantKeyDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValueByKey(keyTestT, key):\n",
    "    valueList=[]\n",
    "    keyTestT = keyTestT.reset_index(drop=True)\n",
    "    for i in tqdm.tqdm(range(len(keyTestT))):\n",
    "        results = json.loads(keyTestT.loc[i, 'results'])\n",
    "        for result in results:\n",
    "            if key in result:\n",
    "                try:\n",
    "                    valueList.append(results[key])\n",
    "                except:\n",
    "                    print('error:', key, i)\n",
    "                    # print(results[key])\n",
    "                    valueList.append(0)                \n",
    "                break\n",
    "    return valueList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得每一个重要key的值\n",
    "def getAllValues(keyTestT, importantKeyList):\n",
    "    valuesDict = {}\n",
    "    for key in importantKeyList:\n",
    "        valuesDict[key] = getValueByKey(keyTestT, key)\n",
    "    return valuesDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesDict = getAllValues(keyTestT, importantKeyList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showDict(valuesDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将valuesDict中的#stream_copy，#stream_triad提取出来，组成DataFrame\n",
    "streamCopyTriad = pd.DataFrame()\n",
    "streamCopyTriad['stream_copy'] = valuesDict['#stream_copy']\n",
    "streamCopyTriad['stream_triad'] = valuesDict['#stream_triad']\n",
    "streamCopyTriad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamCopyTriad.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInput(keyTestT, importantKeyList):\n",
    "    # 根据result中是否有importantKeyList[1]和importantKeyList[3]来提取dimension输入\n",
    "    inputList = []\n",
    "    keys_to_extract = ['cvm_cpu', 'cvm_memory', 'cvm_cpu_qos', 'cvm_os_type']\n",
    "    for i in tqdm.tqdm(range(len(keyTestT))):\n",
    "        results = json.loads(keyTestT.loc[i, 'results'])\n",
    "        has_key1 = False\n",
    "        has_key2 = False\n",
    "        for result in results:\n",
    "            if importantKeyList[3] in result:\n",
    "                has_key1 = True\n",
    "            if importantKeyList[5] in result:\n",
    "                has_key2 = True\n",
    "            if has_key1 == True and has_key2 == True:\n",
    "                # 将dimension中['cvm_cpu', 'cvm_memory', 'cvm_cpu_qos', 'cvm_os_type']这几个字段的值提取出来\n",
    "                dimension = json.loads(keyTestT.loc[i, 'dimension'])\n",
    "                dimension = json.dumps(dimension)\n",
    "                templist=['cvm_cpu', 'cvm_memory', 'cvm_cpu_qos', 'cvm_os_type']\n",
    "                dimensionJson = json.loads(dimension)\n",
    "                for key in dimensionJson:\n",
    "                    if key in templist:\n",
    "                        key_value = dimensionJson[key]\n",
    "                        if key=='cvm_memory':\n",
    "                            key_value=float(key_value.split(' ')[0])\n",
    "                        elif key=='cvm_cpu':\n",
    "                            key_value=float(key_value)\n",
    "                        elif key=='cvm_cpu_qos':\n",
    "                            key_value='true'\n",
    "                        templist[keys_to_extract.index(key)]=key_value\n",
    "                inputList.append(templist)\n",
    "                break\n",
    "    return inputList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputList = getInput(keyTestT, importantKeyList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(inputList))\n",
    "# 打印前5个\n",
    "for i in range(5):\n",
    "    print(inputList[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPic(methodName, y_test, y_pred):\n",
    "    # 创建柱状图\n",
    "    plt.figure(figsize=(80, 15))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.scatter(y_test, y_pred)\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title('{}: True Values vs. Predicted Values'.format(methodName))\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    # 创建样本索引数组\n",
    "    index = np.arange(len(y_test))\n",
    "    # 绘制线性回归模型的折线图\n",
    "    plt.plot(index, y_test, label='True Values')\n",
    "    plt.plot(index, y_pred, label='Predicted Values')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Values')\n",
    "    plt.title('{}: True Values vs. Predicted Values'.format(methodName))\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    # 计算残差\n",
    "    residuals = y_test - y_pred\n",
    "    # 绘制线性回归模型的残差图\n",
    "    plt.scatter(y_test, residuals)\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('{}: True Values vs. Residuals'.format(methodName))\n",
    "    plt.axhline(0, color='red', linestyle='--')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def train(X, y):\n",
    "    # 划分训练集和测试集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "    # 初始化模型\n",
    "    linear_reg = LinearRegression()\n",
    "    decision_tree_reg = DecisionTreeRegressor()\n",
    "    random_forest_reg = RandomForestRegressor(n_estimators=10000, max_depth=5)\n",
    "    svm_reg = SVR(C=1.0, epsilon=0.1)\n",
    "    knn_reg = KNeighborsRegressor(n_neighbors=10, weights='uniform')\n",
    "\n",
    "\n",
    "    # 拟合模型\n",
    "    linear_reg.fit(X_train, y_train)\n",
    "    decision_tree_reg.fit(X_train, y_train)\n",
    "    random_forest_reg.fit(X_train, y_train)\n",
    "    svm_reg.fit(X_train, y_train)\n",
    "    knn_reg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    # 预测\n",
    "    y_pred_linear_reg = linear_reg.predict(X_test)\n",
    "    y_pred_decision_tree_reg = decision_tree_reg.predict(X_test)\n",
    "    y_pred_random_forest_reg = random_forest_reg.predict(X_test)\n",
    "    y_pred_svm_reg = svm_reg.predict(X_test)\n",
    "    y_pred_knn_reg = knn_reg.predict(X_test)\n",
    "\n",
    "\n",
    "    # 计算评估指标\n",
    "    mse_linear_reg = mean_squared_error(y_test, y_pred_linear_reg)\n",
    "    mse_decision_tree_reg = mean_squared_error(y_test, y_pred_decision_tree_reg)\n",
    "    mse_random_forest_reg = mean_squared_error(y_test, y_pred_random_forest_reg)\n",
    "    mse_svm_reg = mean_squared_error(y_test, y_pred_svm_reg)\n",
    "    mse_knn_reg = mean_squared_error(y_test, y_pred_knn_reg)\n",
    "\n",
    "    mae_linear_reg = mean_absolute_error(y_test, y_pred_linear_reg)\n",
    "    mae_decision_tree_reg = mean_absolute_error(y_test, y_pred_decision_tree_reg)\n",
    "    mae_random_forest_reg = mean_absolute_error(y_test, y_pred_random_forest_reg)\n",
    "    mae_svm_reg = mean_absolute_error(y_test, y_pred_svm_reg)\n",
    "    mae_knn_reg = mean_absolute_error(y_test, y_pred_knn_reg)\n",
    "\n",
    "    r2_linear_reg = r2_score(y_test, y_pred_linear_reg)\n",
    "    r2_decision_tree_reg = r2_score(y_test, y_pred_decision_tree_reg)\n",
    "    r2_random_forest_reg = r2_score(y_test, y_pred_random_forest_reg)\n",
    "    r2_svm_reg = r2_score(y_test, y_pred_svm_reg)\n",
    "    r2_knn_reg = r2_score(y_test, y_pred_knn_reg)\n",
    "\n",
    "\n",
    "    # 打印评估指标\n",
    "    # print('Linear Regression MSE:', mse_linear_reg)\n",
    "    # print('Decision Tree Regression MSE:', mse_decision_tree_reg)\n",
    "    # print('Random Forest Regression MSE:', mse_random_forest_reg)\n",
    "    # print('Support Vector Regression MSE:', mse_svm_reg)\n",
    "    # print('KNN Regression MSE:', mse_knn_reg)\n",
    "    # print('\\n')\n",
    "    # print('Linear Regression MAE:', mae_linear_reg)\n",
    "    # print('Decision Tree Regression MAE:', mae_decision_tree_reg)\n",
    "    # print('Random Forest Regression MAE:', mae_random_forest_reg)\n",
    "    # print('Support Vector Regression MAE:', mae_svm_reg)\n",
    "    # print('KNN Regression MAE:', mae_knn_reg)\n",
    "    # print('\\n')\n",
    "    print('Linear Regression R2:', r2_linear_reg)\n",
    "    print('Decision Tree Regression R2:', r2_decision_tree_reg)\n",
    "    print('Random Forest Regression R2:', r2_random_forest_reg)\n",
    "    print('Support Vector Regression R2:', r2_svm_reg)\n",
    "    print('KNN Regression R2:', r2_knn_reg)\n",
    "    print('\\n')\n",
    "\n",
    "    models = ['Linear Regression', 'Decision Tree Regression', 'Random Forest Regression', 'SVM Regression', 'KNN Regression']\n",
    "    y_pred = ['y_pred_linear_reg', 'y_pred_decision_tree_reg', 'y_pred_random_forest_reg', 'y_pred_svm_reg', 'y_pred_knn_reg']\n",
    "    for i in range(len(models)):\n",
    "        showPic(models[i], y_test, eval(y_pred[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(inputList, columns=['cvm_cpu', 'cvm_memory', 'cvm_cpu_qos', 'cvm_os_type'])\n",
    "# 判断['cvm_cpu', 'cvm_memory']这两列的值是否是数字，若不是数字则替换为0\n",
    "df[['cvm_cpu', 'cvm_memory']] = df[['cvm_cpu', 'cvm_memory']].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "# 判断['cvm_cpu_qos', 'cvm_os_type']这两列的值是否是字符串，若不是字符串则转换为字符串\n",
    "df[['cvm_cpu_qos', 'cvm_os_type']] = df[['cvm_cpu_qos', 'cvm_os_type']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cvm_cpu'] = pd.to_numeric(df['cvm_cpu'])\n",
    "df['cvm_memory'] = pd.to_numeric(df['cvm_memory'])\n",
    "one_hot_df = pd.get_dummies(df, columns=['cvm_cpu_qos', 'cvm_os_type'])\n",
    "X = one_hot_df\n",
    "y = streamCopyTriad['stream_copy']\n",
    "\n",
    "# category_features = [x for x in df.columns if df[x].dtype != np.float]\n",
    "# for i in category_features:\n",
    "#     df[i] = df[i].astype('category')\n",
    "\n",
    "train(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下代码暂停使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(df, y , test_size=0.2, random_state=42)\n",
    "\n",
    "# # 创建并训练 LGBMRegressor 模型\n",
    "# lgbm_reg = LGBMRegressor(n_estimators=10000)\n",
    "# lgbm_reg.fit(X_train, y_train)\n",
    "\n",
    "# # 进行预测\n",
    "# y_pred = lgbm_reg.predict(X_test)\n",
    "\n",
    "# # 计算评估指标\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# print(\"Mean Squared Error:\", mse)\n",
    "# mae = mean_absolute_error(y_test, y_pred)\n",
    "# print(\"Mean Absolute Error:\", mae)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "# print(\"R2 Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # one-hot编码\n",
    "# df = pd.DataFrame(inputList, columns=['cvm_cpu', 'cvm_memory', 'cvm_cpu_qos', 'cvm_os_type'])\n",
    "# one_hot_df = pd.get_dummies(df)\n",
    "# # print(one_hot_df)\n",
    "# print(one_hot_df.shape)\n",
    "# # 将one-hot编码后的数据和streamCopyTriad合并\n",
    "# # one_hot_data = pd.concat([one_hot_df, streamCopyTriad], axis=1)\n",
    "# # print(one_hot_data.shape)\n",
    "\n",
    "# X = one_hot_df\n",
    "# y = streamCopyTriad['stream_copy']\n",
    "\n",
    "# train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# encoder = LabelEncoder()\n",
    "# df['cvm_cpu_qos'] = encoder.fit_transform(df['cvm_cpu_qos'])\n",
    "# df['cvm_os_type'] = encoder.fit_transform(df['cvm_os_type'])\n",
    "# # 合并编码后的列与前两列\n",
    "# merged_label_df = df[['cvm_cpu', 'cvm_memory', 'cvm_cpu_qos', 'cvm_os_type']]\n",
    "# print(merged_label_df.shape)\n",
    "# dfL = df.astype(str)\n",
    "# merged_label_df = dfL.apply(encoder.fit_transform)\n",
    "# # 将label编码后的数据和streamCopyTriad合并\n",
    "# # label_data = pd.concat([merged_label_df, streamCopyTriad], axis=1)\n",
    "# # print(label_data.shape)\n",
    "# # label_data.head()\n",
    "\n",
    "# X = merged_label_df\n",
    "# y = streamCopyTriad['stream_copy']\n",
    "\n",
    "# # train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 二进制编码\n",
    "# binary_encoded_df = pd.get_dummies(df, prefix='', prefix_sep='')\n",
    "# # print(binary_encoded_df)\n",
    "# print(binary_encoded_df.shape)\n",
    "# # 将二进制编码后的数据和streamCopyTriad合并\n",
    "# # binary_data = pd.concat([binary_encoded_df, streamCopyTriad], axis=1)\n",
    "# # print(binary_data.shape)\n",
    "\n",
    "# X = binary_encoded_df\n",
    "# y = streamCopyTriad['stream_copy']\n",
    "\n",
    "# # train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 计数count编码\n",
    "# count_encoded_df = df.replace(df.stack().value_counts(normalize=True).to_dict())\n",
    "# # print(count_encoded_df)\n",
    "# print(count_encoded_df.shape)\n",
    "# # 将计数编码后的数据和streamCopyTriad合并\n",
    "# # count_data = pd.concat([count_encoded_df, streamCopyTriad], axis=1)\n",
    "# # print(count_data.shape)\n",
    "# # count_data.head()\n",
    "\n",
    "# X = count_encoded_df\n",
    "# y = streamCopyTriad['stream_copy']\n",
    "\n",
    "# # train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
