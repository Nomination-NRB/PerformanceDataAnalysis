{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import heapq\n",
    "import re\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterFilePath = '../data/filterFile.csv'\n",
    "df = pd.read_csv(filterFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKeyNum(df):\n",
    "    # 提取 results 列中带有 \"#\" 号的字段\n",
    "    pattern = r'#\\w+'  \n",
    "    results = df['results'].str.cat(sep=' ')  # 将所有 results 列的数据合并为一个字符串\n",
    "    hashtags = set(re.findall(pattern, results))  # 使用正则表达式提取带 \"#\" 号的字段，并去重\n",
    "\n",
    "    # 统计每个带 \"#\" 号的字段在整个文件中出现的次数\n",
    "    hashtags_dict = {}\n",
    "    for hashtag in hashtags:\n",
    "        count = results.count(hashtag)\n",
    "        hashtags_dict[hashtag] = count\n",
    "    return hashtags_dict\n",
    "\n",
    "def showKeyNum(hashtags_dict):\n",
    "    # 将字典按值从大到小排序\n",
    "    hashtags_dict_sorted = sorted(hashtags_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 提取排序后的键和值\n",
    "    hashtags_sorted = [item[0] for item in hashtags_dict_sorted]\n",
    "    counts_sorted = [item[1] for item in hashtags_dict_sorted]\n",
    "\n",
    "    # 绘制柱状图\n",
    "    plt.figure(figsize=(180, 25))  \n",
    "    plt.bar(hashtags_sorted, counts_sorted)  \n",
    "    plt.xticks(rotation=90)  \n",
    "    plt.xlabel('Hashtags')  \n",
    "    plt.ylabel('Counts')  \n",
    "    plt.title('Hashtags Counts')  \n",
    "    plt.show()\n",
    "\n",
    "def getTop10Key(hashtags_dict):\n",
    "    # 获取字典中数量前十的字段\n",
    "    top_n = 10  # 自定义获取前几个字段\n",
    "    top_n_fields = heapq.nlargest(top_n, hashtags_dict, key=hashtags_dict.get)\n",
    "    \n",
    "    print(\"数量前十的字段：\")\n",
    "    for field in top_n_fields:\n",
    "        print(\"字段名: {:<30s} 出现次数: {:d}\".format(field, hashtags_dict[field]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_dict=getKeyNum(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_output(Tdf, field):\n",
    "    df = Tdf.copy()\n",
    "    input_list = []\n",
    "    output_list = []\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        if field in row['results']:\n",
    "            result_dict = json.loads(row['results'])\n",
    "            if field in result_dict:\n",
    "                dimension_dict = json.loads(row['dimension'])\n",
    "                input_list.append([\n",
    "                    float(dimension_dict['cvm_cpu']) if 'cvm_cpu' in dimension_dict else -1,\n",
    "                    float(dimension_dict['cvm_memory'].split()[0]) if 'cvm_memory' in dimension_dict else -1,\n",
    "                    dimension_dict.get('cvm_cpu_qos', 'false') == 'true',\n",
    "                    dimension_dict.get('cvm_os_type', ''),\n",
    "                    row['results_key']\n",
    "                ])\n",
    "                output_list.append(result_dict[field])\n",
    "    input_df = pd.DataFrame(input_list, columns=['cvm_cpu', 'cvm_memory', 'cvm_cpu_qos', 'cvm_os_type', 'results_key'])\n",
    "    output_df = pd.DataFrame({field: output_list})\n",
    "    return input_df, output_df\n",
    "\n",
    "\n",
    "def get_input_output_Speed(Tdf, field):\n",
    "    # 选择需要提取的字段\n",
    "    df = Tdf.copy()\n",
    "    keys_to_extract = ['cvm_cpu', 'cvm_memory', 'cvm_cpu_qos', 'cvm_os_type']\n",
    "\n",
    "    # 对dimension列进行预处理\n",
    "    df['dimension'] = df['dimension'].apply(lambda x: json.loads(x))\n",
    "    for key in keys_to_extract:\n",
    "        df[key] = df['dimension'].apply(lambda x: x.get(key, None))\n",
    "\n",
    "    # 对cvm_memory和cvm_cpu进行数值化处理\n",
    "    df[['cvm_cpu', 'cvm_memory']] = df[['cvm_cpu', 'cvm_memory']].apply(pd.to_numeric, errors='coerce').fillna(-1)\n",
    "\n",
    "    # 筛选出符合要求的行\n",
    "    df_filtered = df[df['results'].str.contains(field)]\n",
    "\n",
    "    # 从results中提取出field对应的值\n",
    "    df_output = pd.DataFrame(df_filtered['results'].apply(lambda x: json.loads(x)).tolist(), index=df_filtered.index)\n",
    "    df_output = df_output[field]\n",
    "\n",
    "    # 将input和output分别转成dataframe\n",
    "    df_input = df_filtered[keys_to_extract + ['results_key']]\n",
    "    df_output = pd.DataFrame(df_output)\n",
    "\n",
    "    return df_input, df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'#fio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32me:\\test\\envs\\pytorch\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2897\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2898\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2899\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '#fio'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-62de83a7b6d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0myour_field\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'#fio'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# inPutDF, outPutDF = get_input_output(df, your_field)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0minPutDF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutPutDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_input_output_Speed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myour_field\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-f0f23e921cac>\u001b[0m in \u001b[0;36mget_input_output_Speed\u001b[1;34m(Tdf, field)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m# 从results中提取出field对应的值\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mdf_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_filtered\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'results'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_filtered\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mdf_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m# 将input和output分别转成dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\test\\envs\\pytorch\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2904\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2905\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2906\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2907\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\test\\envs\\pytorch\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2898\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2900\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2901\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2902\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '#fio'"
     ]
    }
   ],
   "source": [
    "your_field = '#fio'\n",
    "# inPutDF, outPutDF = get_input_output(df, your_field)\n",
    "inPutDF, outPutDF = get_input_output_Speed(df, your_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 5)\n",
      "(0, 1)\n"
     ]
    }
   ],
   "source": [
    "print(inPutDF.shape)\n",
    "print(outPutDF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import pickle\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPic(methodName, y_test, y_pred):\n",
    "    # 创建柱状图\n",
    "    plt.figure(figsize=(80, 15))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    # 创建样本索引数组\n",
    "    index = np.arange(len(y_test))\n",
    "    # 绘制线性回归模型的折线图\n",
    "    plt.plot(index, y_test, label='True Values')\n",
    "    plt.plot(index, y_pred, label='Predicted Values')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Values')\n",
    "    plt.title('{}: True Values vs. Predicted Values'.format(methodName))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def showCoef(linear_reg, decision_tree_reg, random_forest_reg):\n",
    "    # 绘制特征重要性条形图\n",
    "    if len(linear_reg.coef_) > 1:\n",
    "        coef_abs = np.abs(linear_reg.coef_)\n",
    "        sorted_idx = np.argsort(coef_abs)\n",
    "        plt.figure(figsize=(150, 20))\n",
    "        plt.bar(range(len(coef_abs)), coef_abs[sorted_idx])\n",
    "        plt.xticks(range(len(coef_abs)), sorted_idx)\n",
    "        plt.xlabel('Feature Index')\n",
    "        plt.ylabel('Feature Importance (by coefficient absolute value)')\n",
    "        plt.title('Linear Regression - Feature Importance')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('Linear Regression - No feature importance to plot')\n",
    "\n",
    "    if len(decision_tree_reg.feature_importances_) > 1:\n",
    "        feature_importances = decision_tree_reg.feature_importances_\n",
    "        sorted_idx = np.argsort(feature_importances)\n",
    "        plt.figure(figsize=(150, 20))\n",
    "        plt.bar(range(len(feature_importances)), feature_importances[sorted_idx])\n",
    "        plt.xticks(range(len(feature_importances)), sorted_idx)\n",
    "        plt.xlabel('Feature Index')\n",
    "        plt.ylabel('Feature Importance')\n",
    "        plt.title('Decision Tree - Feature Importance')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('Decision Tree - No feature importance to plot')\n",
    "\n",
    "    if len(random_forest_reg.feature_importances_) > 1:\n",
    "        feature_importances = random_forest_reg.feature_importances_\n",
    "        sorted_idx = np.argsort(feature_importances)\n",
    "        plt.figure(figsize=(150, 20))\n",
    "        plt.bar(range(len(feature_importances)), feature_importances[sorted_idx])\n",
    "        plt.xticks(range(len(feature_importances)), sorted_idx)\n",
    "        plt.xlabel('Feature Index')\n",
    "        plt.ylabel('Feature Importance')\n",
    "        plt.title('Random Forest - Feature Importance')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('Random Forest - No feature importance to plot')\n",
    "\n",
    "\n",
    "def getCoef(linear_reg, decison_tree_reg, random_forest_reg):\n",
    "    result = []\n",
    "    if len(linear_reg.coef_) > 1:\n",
    "        coef_abs = np.abs(linear_reg.coef_)\n",
    "        sorted_idx = np.argsort(coef_abs)\n",
    "        # 将最后10个特征的索引存入templist中\n",
    "        templist = []\n",
    "        for i in range(1, 11):\n",
    "            templist.append(sorted_idx[-i])\n",
    "        result.append(templist)\n",
    "    else:\n",
    "        result.append([])\n",
    "    if len(decison_tree_reg.feature_importances_) > 1:\n",
    "        feature_importances = decison_tree_reg.feature_importances_\n",
    "        sorted_idx = np.argsort(feature_importances)\n",
    "        # 将最后10个特征的索引存入templist中\n",
    "        templist = []\n",
    "        for i in range(1, 11):\n",
    "            templist.append(sorted_idx[-i])\n",
    "        result.append(templist)\n",
    "    else:\n",
    "        result.append([])\n",
    "    if len(random_forest_reg.feature_importances_) > 1:\n",
    "        feature_importances = random_forest_reg.feature_importances_\n",
    "        sorted_idx = np.argsort(feature_importances)\n",
    "        # 将最后10个特征的索引存入templist中\n",
    "        templist = []\n",
    "        for i in range(1, 11):\n",
    "            templist.append(sorted_idx[-i])\n",
    "        result.append(templist)\n",
    "    else:\n",
    "        result.append([])\n",
    "    return result\n",
    "\n",
    "\n",
    "def getFeatureName(result, inputDF):\n",
    "    feature_names = inputDF.columns\n",
    "    linear_reg_indices = result[0]  # 线性回归模型的特征索引\n",
    "    decision_tree_indices = result[1]  # 决策树模型的特征索引\n",
    "    random_forest_indices = result[2]  # 随机森林模型的特征索引\n",
    "\n",
    "    linear_reg_feature_names = feature_names[linear_reg_indices].tolist()  # 线性回归模型的特征列名列表\n",
    "    decision_tree_feature_names = feature_names[decision_tree_indices].tolist()  # 决策树模型的特征列名列表\n",
    "    random_forest_feature_names = feature_names[random_forest_indices].tolist()  # 随机森林模型的特征列名列表\n",
    "    \n",
    "    linear_reg_features = inputDF[linear_reg_feature_names]  # 线性回归模型的特征值\n",
    "    decision_tree_features = inputDF[decision_tree_feature_names]  # 决策树模型的特征值\n",
    "    random_forest_features = inputDF[random_forest_feature_names]  # 随机森林模型的特征值\n",
    "\n",
    "\n",
    "def showDecisonTree(decision_tree_reg, flag=False):\n",
    "    if flag:    \n",
    "        plt.figure(figsize=(20, 10))  # 设置画布大小\n",
    "        plot_tree(decision_tree_reg, max_depth=2, feature_names=None, filled=True, rounded=True)  # 设置最大深度为2，可以根据需要调整\n",
    "        plt.title('Decision Tree')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def showRandomForest(random_forest_reg, flag=False):\n",
    "    if flag:\n",
    "        if len(random_forest_reg.feature_importances_) > 1:\n",
    "            # 获取特征重要性\n",
    "            feature_importances = random_forest_reg.feature_importances_\n",
    "            # 选择特征重要性最高的决策树\n",
    "            best_tree_index = np.argmax(feature_importances)\n",
    "            if best_tree_index >= random_forest_reg.n_estimators:\n",
    "                best_tree_index = random_forest_reg.n_estimators - 1\n",
    "\n",
    "            plt.figure(figsize=(20, 10))\n",
    "            plot_tree(random_forest_reg.estimators_[best_tree_index], max_depth=2, feature_names=None, filled=True, rounded=True)\n",
    "            plt.title('Best Decision Tree')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print('Random Forest - No feature importance to plot')\n",
    "\n",
    "\n",
    "def getModel(X_train, y_train, save_path):\n",
    "    # 初始化模型\n",
    "    linear_reg = LinearRegression()\n",
    "    decision_tree_reg = DecisionTreeRegressor()\n",
    "    random_forest_reg = RandomForestRegressor(n_estimators=100, max_depth=5)\n",
    "    svm_reg = SVR(C=1.0, epsilon=0.1)\n",
    "    knn_reg = KNeighborsRegressor(n_neighbors=10, weights='uniform')\n",
    "\n",
    "    # 拟合模型\n",
    "    linear_reg.fit(X_train, y_train)\n",
    "    decision_tree_reg.fit(X_train, y_train)\n",
    "    random_forest_reg.fit(X_train, y_train)\n",
    "    svm_reg.fit(X_train, y_train)\n",
    "    knn_reg.fit(X_train, y_train)\n",
    "\n",
    "    # 保存模型\n",
    "    models = {\n",
    "        'linear_reg': linear_reg,\n",
    "        'decision_tree_reg': decision_tree_reg,\n",
    "        'random_forest_reg': random_forest_reg,\n",
    "        'svm_reg': svm_reg,\n",
    "        'knn_reg': knn_reg\n",
    "    }\n",
    "    # save_path = 'model/'\n",
    "    for model_name, model in models.items():\n",
    "        with open(save_path + model_name + '.pkl', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "    return linear_reg, decision_tree_reg, random_forest_reg, svm_reg, knn_reg\n",
    "\n",
    "\n",
    "def train(X, y, save_path):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "    # 获取模型\n",
    "    linear_reg, decision_tree_reg, random_forest_reg, svm_reg, knn_reg = getModel(X_train, y_train, save_path)\n",
    "\n",
    "    # 预测\n",
    "    y_pred_linear_reg = linear_reg.predict(X_test)\n",
    "    y_pred_decision_tree_reg = decision_tree_reg.predict(X_test)\n",
    "    y_pred_random_forest_reg = random_forest_reg.predict(X_test)\n",
    "    y_pred_svm_reg = svm_reg.predict(X_test)\n",
    "    y_pred_knn_reg = knn_reg.predict(X_test)\n",
    "\n",
    "    # 计算评估指标\n",
    "    r2_linear_reg = r2_score(y_test, y_pred_linear_reg)\n",
    "    r2_decision_tree_reg = r2_score(y_test, y_pred_decision_tree_reg)\n",
    "    r2_random_forest_reg = r2_score(y_test, y_pred_random_forest_reg)\n",
    "    r2_svm_reg = r2_score(y_test, y_pred_svm_reg)\n",
    "    r2_knn_reg = r2_score(y_test, y_pred_knn_reg)\n",
    "\n",
    "    print('{:<30} {:>10}'.format('Linear Regression R2:', '{:.8f}'.format(r2_linear_reg)))\n",
    "    print('{:<30} {:>10}'.format('Decision Tree R2:', '{:.8f}'.format(r2_decision_tree_reg)))\n",
    "    print('{:<30} {:>10}'.format('Random Forest R2:', '{:.8f}'.format(r2_random_forest_reg)))\n",
    "    print('{:<30} {:>10}'.format('SVM R2:', '{:.8f}'.format(r2_svm_reg)))\n",
    "    print('{:<30} {:>10}'.format('KNN R2:', '{:.8f}'.format(r2_knn_reg)))\n",
    "    print('\\n')\n",
    "\n",
    "    models = ['Linear Regression', 'Decision Tree Regression', 'Random Forest Regression', 'SVM Regression', 'KNN Regression']\n",
    "    y_pred = ['y_pred_linear_reg', 'y_pred_decision_tree_reg', 'y_pred_random_forest_reg', 'y_pred_svm_reg', 'y_pred_knn_reg']\n",
    "    for i in range(len(models)):\n",
    "        showPic(models[i], y_test, eval(y_pred[i]))\n",
    "\n",
    "    return linear_reg, decision_tree_reg, random_forest_reg, svm_reg, knn_reg\n",
    "\n",
    "\n",
    "def plot_learning_curve(model, X, y):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=5)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"b\",\n",
    "             label=\"Cross-validation score\")\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                     color=\"g\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def judgeResult(model, X_train, y_train, X_test, y_test):\n",
    "    # 在训练集和测试集上计算评估指标\n",
    "    y_train = y_train.values\n",
    "    y_test = y_test.values\n",
    "    y_train = y_train.ravel()\n",
    "    y_test = y_test.ravel()\n",
    "\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "    print('Model: {}'.format(model.__class__.__name__))\n",
    "    print('{:<30} {:>10}'.format('Train R2:', '{:.8f}'.format(r2_train)))\n",
    "    print('{:<30} {:>10}'.format('Test R2:', '{:.8f}'.format(r2_test)))\n",
    "    \n",
    "    # 绘制学习曲线\n",
    "    # plot_learning_curve(model, X_train, y_train)\n",
    "\n",
    "    # 判断过拟合\n",
    "    if r2_test >= r2_train:\n",
    "        print(\"模型可能存在过拟合的问题\\n\\n\")\n",
    "    else:\n",
    "        print(\"模型可能没有过拟合的问题\\n\\n\")\n",
    "\n",
    "\n",
    "def trainOverFit(X, y, save_path):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "    # 获取模型\n",
    "    linear_reg, decision_tree_reg, random_forest_reg, svm_reg, knn_reg = getModel(X_train, y_train, save_path)\n",
    "\n",
    "    # 预测\n",
    "\n",
    "    y_pred_linear_reg = linear_reg.predict(X_test)\n",
    "    y_pred_decision_tree_reg = decision_tree_reg.predict(X_test)\n",
    "    y_pred_random_forest_reg = random_forest_reg.predict(X_test)\n",
    "    y_pred_svm_reg = svm_reg.predict(X_test)\n",
    "    y_pred_knn_reg = knn_reg.predict(X_test)\n",
    "\n",
    "    # 计算评估指标\n",
    "    r2_linear_reg = r2_score(y_test, y_pred_linear_reg)\n",
    "    r2_decision_tree_reg = r2_score(y_test, y_pred_decision_tree_reg)\n",
    "    r2_random_forest_reg = r2_score(y_test, y_pred_random_forest_reg)\n",
    "    r2_svm_reg = r2_score(y_test, y_pred_svm_reg)\n",
    "    r2_knn_reg = r2_score(y_test, y_pred_knn_reg)\n",
    "\n",
    "    print('{:<30} {:>10}'.format('Linear Regression R2:', '{:.8f}'.format(r2_linear_reg)))\n",
    "    print('{:<30} {:>10}'.format('Decision Tree R2:', '{:.8f}'.format(r2_decision_tree_reg)))\n",
    "    print('{:<30} {:>10}'.format('Random Forest R2:', '{:.8f}'.format(r2_random_forest_reg)))\n",
    "    print('{:<30} {:>10}'.format('SVM R2:', '{:.8f}'.format(r2_svm_reg)))\n",
    "    print('{:<30} {:>10}'.format('KNN R2:', '{:.8f}'.format(r2_knn_reg)))\n",
    "    print('\\n')\n",
    "\n",
    "    models = ['Linear Regression', 'Decision Tree Regression', 'Random Forest Regression', 'SVM Regression', 'KNN Regression']\n",
    "    y_pred = [y_pred_linear_reg, y_pred_decision_tree_reg, y_pred_random_forest_reg, y_pred_svm_reg, y_pred_knn_reg]\n",
    "    # for i in range(len(models)):\n",
    "    #     showPic(models[i], y_test, y_pred[i])\n",
    "\n",
    "    getModelList = ['linear_reg', 'decision_tree_reg', 'random_forest_reg', 'svm_reg', 'knn_reg']\n",
    "\n",
    "    for i in range(len(models)):\n",
    "        judgeResult(eval(getModelList[i]), X_train, y_train, X_test, y_test)\n",
    "\n",
    "    return linear_reg, decision_tree_reg, random_forest_reg, svm_reg, knn_reg\n",
    "\n",
    "\n",
    "def getFeatureName(X, result):\n",
    "    feature_names = X.columns\n",
    "    linear_reg_indices = result[0]  # 线性回归模型的特征索引\n",
    "    decision_tree_indices = result[1]  # 决策树模型的特征索引\n",
    "    random_forest_indices = result[2]  # 随机森林模型的特征索引\n",
    "    resDict={}\n",
    "    if len(linear_reg_indices) > 0:\n",
    "        print('linear_reg_index: ', linear_reg_indices)\n",
    "        linear_reg_feature_names = feature_names[linear_reg_indices].tolist()\n",
    "        print('linear_reg_feature_names: ', linear_reg_feature_names)\n",
    "        resDict['linear_reg_feature_names']=linear_reg_feature_names\n",
    "    if len(decision_tree_indices) > 0:\n",
    "        print('decision_tree_reg_index: ', decision_tree_indices)\n",
    "        decision_tree_feature_names = feature_names[decision_tree_indices].tolist()\n",
    "        print('decision_tree_feature_names: ', decision_tree_feature_names)\n",
    "        resDict['decision_tree_feature_names']=decision_tree_feature_names\n",
    "    if len(random_forest_indices) > 0:\n",
    "        print('random_forest_reg_index: ', random_forest_indices)\n",
    "        random_forest_feature_names = feature_names[random_forest_indices].tolist()\n",
    "        print('random_forest_feature_names: ', random_forest_feature_names)\n",
    "        resDict['random_forest_feature_names']=random_forest_feature_names\n",
    "    return resDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15400, 418)\n",
      "(0, 1)\n"
     ]
    }
   ],
   "source": [
    "inPutDF['cvm_cpu'] = pd.to_numeric(inPutDF['cvm_cpu'])\n",
    "inPutDF['cvm_memory'] = pd.to_numeric(inPutDF['cvm_memory'])\n",
    "one_hot_df = pd.get_dummies(inPutDF, columns=['cvm_cpu_qos', 'cvm_os_type','results_key'])\n",
    "X = one_hot_df\n",
    "y = outPutDF\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [15400, 0]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-308969764b1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# linear_reg, decision_tree_reg, random_forest_reg, svm_reg, knn_reg = trainOverFit(X, y, save_path)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlinear_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecision_tree_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_forest_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msvm_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mknn_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-66fd968ee52b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(X, y, save_path)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[1;31m# 获取模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\test\\envs\\pytorch\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2170\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"At least one array required as input\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2172\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2174\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\test\\envs\\pytorch\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    354\u001b[0m     \"\"\"\n\u001b[0;32m    355\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\test\\envs\\pytorch\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 320\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [15400, 0]"
     ]
    }
   ],
   "source": [
    "save_path = 'model/{}'.format(your_field)\n",
    "# 判断是否有这个文件夹，没有就创建 \n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# linear_reg, decision_tree_reg, random_forest_reg, svm_reg, knn_reg = trainOverFit(X, y, save_path)\n",
    "\n",
    "linear_reg, decision_tree_reg, random_forest_reg, svm_reg, knn_reg = train(X, y, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg, decision_tree_reg, random_forest_reg, svm_reg, knn_reg = trainOverFit(X, y, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印决策树\n",
    "showDecisonTree(decision_tree_reg, flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印随机森林\n",
    "showRandomForest(random_forest_reg, flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultCoef = getCoef(linear_reg, decision_tree_reg, random_forest_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureImportDict=getFeatureName(X, resultCoef)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
